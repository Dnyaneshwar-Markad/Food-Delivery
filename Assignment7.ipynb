{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d104bdb-5685-4c99-9448-ae13d5192ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order_ID                      0\n",
      "Customer_Location             0\n",
      "Restaurant_Location           0\n",
      "Distance                      0\n",
      "Weather_Conditions            0\n",
      "Traffic_Conditions            0\n",
      "Delivery_Person_Experience    0\n",
      "Order_Priority                0\n",
      "Order_Time                    0\n",
      "Vehicle_Type                  0\n",
      "Restaurant_Rating             0\n",
      "Customer_Rating               0\n",
      "Delivery_Time                 0\n",
      "Order_Cost                    0\n",
      "Tip_Amount                    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('Food_Delivery_Time_Prediction.csv')\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55b01ca2-ed46-48ca-a3f6-e8743c6b4b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()  # or df.fillna(df.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fbc4d75-fed9-46a3-b343-ac1b96904e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0  # Earth radius in km\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "# Extract lat/long from strings\n",
    "df['Customer_Lat'] = df['Customer_Location'].str.extract(r'\\((.*?),')[0].astype(float)\n",
    "df['Customer_Lon'] = df['Customer_Location'].str.extract(r', (.*?)\\)')[0].astype(float)\n",
    "df['Restaurant_Lat'] = df['Restaurant_Location'].str.extract(r'\\((.*?),')[0].astype(float)\n",
    "df['Restaurant_Lon'] = df['Restaurant_Location'].str.extract(r', (.*?)\\)')[0].astype(float)\n",
    "\n",
    "# Apply Haversine formula\n",
    "df['Calculated_Distance'] = df.apply(\n",
    "    lambda x: haversine(x['Customer_Lat'], x['Customer_Lon'], x['Restaurant_Lat'], x['Restaurant_Lon']),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ffa2ce80-6d8c-4f8b-9076-0bb337cd393e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "for col in ['Weather_Conditions', 'Traffic_Conditions', 'Order_Priority', 'Order_Time', 'Vehicle_Type']:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76121a6a-5d75-4ae0-b7a2-2ad9b900a4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "numerical_cols = ['Distance', 'Delivery_Person_Experience', 'Restaurant_Rating', 'Customer_Rating', 'Order_Cost', 'Tip_Amount']\n",
    "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b758bb5-20f7-4cbf-9c48-3ad4b1e2adf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume \"Fast\" if delivery time < median, else \"Delayed\"\n",
    "median_delivery_time = df['Delivery_Time'].median()\n",
    "df['Delivery_Status'] = np.where(df['Delivery_Time'] < median_delivery_time, 'Fast', 'Delayed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6294c5d8-c94e-4afd-95bb-4f2c773e06e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: CNN Implementation\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, MaxPooling1D, Flatten, Dense, Reshape\n",
    "\n",
    "# Create synthetic \"image\" data (distance, weather, traffic, etc.)\n",
    "X = df[['Calculated_Distance', 'Weather_Conditions', 'Traffic_Conditions']].values\n",
    "X = X.reshape(-1, 3, 1, 1)  # Reshape for CNN (batch, height, width, channels)\n",
    "\n",
    "# Encode target\n",
    "y = df['Delivery_Status'].apply(lambda x: 1 if x == 'Fast' else 0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1a462413-6ac3-454f-9b45-5c86db8428ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    Reshape((3, 1, 1)),  # Explicitly reshape to (3, 1, 1)\n",
    "    Conv2D(32, (2, 1), activation='relu'),  # Kernel size (2, 1)\n",
    "    MaxPooling2D((1, 1)),  # Minimal pooling to avoid dimension collapse\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "25d49034-5071-4198-a099-1d4b2af3cb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000029BD3757BA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 103ms/stepWARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000029BD3757BA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "Mean CV Accuracy: 0.44\n"
     ]
    }
   ],
   "source": [
    "#  Phase 3: Model Evaluation & Validation\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Initialize KFold\n",
    "kf = KFold(n_splits=5)\n",
    "accuracies = []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # Define and compile the model inside the loop\n",
    "    model = tf.keras.Sequential([\n",
    "        Reshape((3, 1)),  # Reshape to (samples, 3, 1)\n",
    "        Conv1D(32, kernel_size=2, activation='relu'),\n",
    "        MaxPooling1D(pool_size=1),\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=5, verbose=0)\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(f\"Mean CV Accuracy: {np.mean(accuracies):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fdb33b56-ea4c-4f3d-88b1-b278228bcc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - accuracy: 0.5577 - loss: 23.8663 - val_accuracy: 0.3750 - val_loss: 15.3034\n",
      "Epoch 2/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5383 - loss: 8.1274 - val_accuracy: 0.6250 - val_loss: 4.4550\n",
      "Epoch 3/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5065 - loss: 6.6993 - val_accuracy: 0.6250 - val_loss: 3.9915\n",
      "Epoch 4/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4040 - loss: 4.2950 - val_accuracy: 0.3750 - val_loss: 4.3078\n",
      "Epoch 5/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5417 - loss: 3.1724 - val_accuracy: 0.3750 - val_loss: 2.7166\n",
      "Epoch 6/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5548 - loss: 1.1361 - val_accuracy: 0.6250 - val_loss: 1.7070\n",
      "Epoch 7/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5096 - loss: 1.7605 - val_accuracy: 0.3750 - val_loss: 1.8999\n",
      "Epoch 8/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5139 - loss: 1.5173 - val_accuracy: 0.4500 - val_loss: 0.7201\n",
      "Epoch 9/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5187 - loss: 0.8769 - val_accuracy: 0.4500 - val_loss: 0.7653\n",
      "Epoch 10/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5895 - loss: 0.7951 - val_accuracy: 0.6250 - val_loss: 0.6805\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "Confusion Matrix:\n",
      " [[25  0]\n",
      " [15  0]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      1.00      0.77        25\n",
      "           1       0.00      0.00      0.00        15\n",
      "\n",
      "    accuracy                           0.62        40\n",
      "   macro avg       0.31      0.50      0.38        40\n",
      "weighted avg       0.39      0.62      0.48        40\n",
      "\n",
      "ROC-AUC: 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "# Train final model on full data (optional)\n",
    "final_model = tf.keras.Sequential([\n",
    "    Reshape((3, 1)),\n",
    "    Conv1D(32, kernel_size=2, activation='relu'),\n",
    "    MaxPooling1D(pool_size=1),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "final_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "final_model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Generate predictions\n",
    "y_pred = (final_model.predict(X_test) > 0.5).astype(int)\n",
    "\n",
    "# Metrics\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3d2d40-f563-4fd7-8d69-5652595d34ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Food Delivery Time Prediction: Final Report\n",
    "\n",
    "# ## 1. Methodology\n",
    "\n",
    "# ### Data Preparation\n",
    "# - **Dataset**: The data was loaded from `Food_Delivery_Time_Prediction.csv`, containing order and delivery information such as locations, order priority, ratings, time, cost, tips, weather, and traffic conditions.\n",
    "# - **Missing Values**: Checked and ensured there were no missing values; alternatively, rows with missing data were dropped.\n",
    "# - **Feature Engineering**:\n",
    "#   - **Location Extraction**: Latitude and longitude were extracted from `Customer_Location` and `Restaurant_Location` string columns.\n",
    "#   - **Distance Calculation**: The Haversine formula was used to calculate the actual distance between customer and restaurant.\n",
    "# - **Encoding & Scaling**:\n",
    "#   - Categorical variables (weather, traffic, priority, etc.) were encoded using `LabelEncoder`.\n",
    "#   - Numerical features (distance, experience, ratings, cost, tips) were standardized with `StandardScaler`.\n",
    "# - **Target Variable**: Delivery times were labeled as `\"Fast\"` or `\"Delayed\"` based on whether they were below or above the median delivery time.\n",
    "\n",
    "# ### Model Construction: Convolutional Neural Network (CNN)\n",
    "# - **Feature Selection**: Used `Calculated_Distance`, `Weather_Conditions`, and `Traffic_Conditions` as input features for the CNN.\n",
    "# - **Input Reshaping**: Data was reshaped to mimic an image-like structure suitable for CNNs.\n",
    "# - **Architecture**:\n",
    "#   - Used either a 1D or 2D CNN with convolution, pooling, flattening, and dense layers.\n",
    "#   - Output layer used sigmoid activation for binary classification (`Fast` vs. `Delayed`).\n",
    "# - **Training**: The model was compiled with the Adam optimizer and binary cross-entropy loss.\n",
    "\n",
    "# ## 2. Model Validation Techniques\n",
    "\n",
    "# - **Cross-Validation**: Employed 5-fold cross-validation using `KFold` from scikit-learn. In each fold:\n",
    "#   - The model was initialized, trained, and evaluated on the fold's train/test split.\n",
    "#   - Accuracy scores from each fold were averaged to report mean cross-validation accuracy.\n",
    "# - **Final Training/Evaluation**: After cross-validation, a final model was trained on the full dataset (with an 80/20 validation split) for final performance metrics.\n",
    "\n",
    "# ## 3. Model Performance\n",
    "\n",
    "# ### Cross-Validation Results\n",
    "# - **Mean CV Accuracy**: ~0.44 (from code output). This indicates the model's accuracy in distinguishing fast vs. delayed deliveries is only modestly better than random guessing.\n",
    "\n",
    "# ### Final Model Evaluation\n",
    "# - **Epoch-wise Training**: The model was trained for multiple epochs, with accuracy and loss monitored per epoch.\n",
    "\n",
    "# ## 4. Key Findings & Discussion\n",
    "\n",
    "# - **CNN Model Limitations**:\n",
    "#   - The selected features and small input shape (3x1) severely limit the learning ability of a CNN, which excels with spatial or sequential patterns in larger datasets.\n",
    "#   - The model fell into predicting only the majority class, failing to capture the minority class (\"Delayed\" deliveries).\n",
    "#   - ROC-AUC and recall metrics confirm poor discriminatory power.\n",
    "\n",
    "# - **Model Validation Insights**:\n",
    "#   - Cross-validation revealed instability and low accuracy, suggesting the problem or data is not suitable for a CNN-based approach with the current feature set.\n",
    "#   - Potential overfitting or underfitting, possibly due to simplistic features or imbalanced target labels.\n",
    "\n",
    "# ## 5. Recommendations\n",
    "\n",
    "# - **Feature Engineering**: Consider additional features (e.g., time of day, restaurant/customer clusters, more granular weather/traffic info).\n",
    "# - **Model Selection**: Try alternative models more suitable for tabular data (e.g., Random Forest, Gradient Boosting, Logistic Regression).\n",
    "# - **Class Imbalance**: Apply techniques like oversampling, class weighting, or threshold tuning to handle imbalanced classes.\n",
    "# - **Deep Learning for Tabular Data**: CNNs are generally not optimal for small, non-sequential, tabular datasets unless features can be meaningfully arranged spatially.\n",
    "\n",
    "# ## 6. Conclusion\n",
    "\n",
    "# The CNN approach for food delivery time classification on this tabular dataset yielded subpar performance, highlighting the importance of model-data alignment and comprehensive feature engineering. Future work should focus on enhancing data representation, addressing class imbalance, and exploring more appropriate model architectures for this problem.\n",
    "\n",
    "# ---\n",
    "# **Prepared by:**  \n",
    "# Dnyaneshwar-Markad  \n",
    "# Date: 2025-05-28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde84d8c-4732-4a9f-b986-71e19712a922",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
